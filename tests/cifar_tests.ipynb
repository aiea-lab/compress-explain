{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from IPython import embed\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get10(batch_size, data_root='/tmp/public_dataset/pytorch', train=True, val=True, **kwargs):\n",
    "    data_root = os.path.expanduser(os.path.join(data_root, 'cifar10-data'))\n",
    "    num_workers = kwargs.setdefault('num_workers', 1)\n",
    "    kwargs.pop('input_size', None)\n",
    "    print(\"Building CIFAR-10 data loader with {} workers\".format(num_workers))\n",
    "    ds = []\n",
    "    if train:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            datasets.CIFAR10(\n",
    "                root=data_root, train=True, download=True,\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.Pad(4),\n",
    "                    transforms.RandomCrop(32),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                ])),\n",
    "            batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        ds.append(train_loader)\n",
    "    if val:\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.CIFAR10(\n",
    "                root=data_root, train=False, download=True,\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                ])),\n",
    "            batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        ds.append(test_loader)\n",
    "    ds = ds[0] if len(ds) == 1 else ds\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    'cifar10': 'http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/cifar10-d875770b.pth',\n",
    "    'cifar100': 'http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/cifar100-3a55a987.pth',\n",
    "}\n",
    "\n",
    "class CIFAR(nn.Module):\n",
    "    def __init__(self, features, n_channel, num_classes):\n",
    "        super(CIFAR, self).__init__()\n",
    "        assert isinstance(features, nn.Sequential), type(features)\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_channel, num_classes)\n",
    "        )\n",
    "        print(self.features)\n",
    "        print(self.classifier)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for i, v in enumerate(cfg):\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            padding = v[1] if isinstance(v, tuple) else 1\n",
    "            out_channels = v[0] if isinstance(v, tuple) else v\n",
    "            conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(out_channels, affine=False), nn.ReLU()]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU()]\n",
    "            in_channels = out_channels\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def cifar10(n_channel, pretrained=None):\n",
    "    cfg = [n_channel, n_channel, 'M', 2*n_channel, 2*n_channel, 'M', 4*n_channel, 4*n_channel, 'M', (8*n_channel, 0), 'M']\n",
    "    layers = make_layers(cfg, batch_norm=True)\n",
    "    model = CIFAR(layers, n_channel=8*n_channel, num_classes=10)\n",
    "    if pretrained is not None:\n",
    "        m = model_zoo.load_url(model_urls['cifar10'])\n",
    "        state_dict = m.state_dict() if isinstance(m, nn.Module) else m\n",
    "        assert isinstance(state_dict, (dict, OrderedDict)), type(state_dict)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building CIFAR-10 data loader with 1 workers\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (9): ReLU()\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (12): ReLU()\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (16): ReLU()\n",
      "  (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (19): ReLU()\n",
      "  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (22): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (23): ReLU()\n",
      "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get10(batch_size=200, num_workers=1)\n",
    "model = cifar10(128, pretrained=True)\n",
    "# model = cifar10(128, pretrained=None)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.001\n",
    "# wd = 0.00\n",
    "# epochs = 150\n",
    "log_interval = 100\n",
    "test_interval = 5\n",
    "# decreasing_lr = '80,120'\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "# decreasing_lr = list(map(int, decreasing_lr.split(',')))\n",
    "# print('decreasing_lr: ' + str(decreasing_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, conv_prune=0.3, lin_prune=0.6):\n",
    "    for name, module in model.named_modules():\n",
    "        # prune 30% of connections in all 2D-conv layers\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name='weight', amount=conv_prune)\n",
    "        # prune 60% of connections in all linear layers\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=lin_prune)\n",
    "\n",
    "    print(dict(model.named_buffers()).keys())  # to verify that all masks exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=150, lr=0.001, decreasing_lr='80,120', wd=0):\n",
    "    best_acc = 0\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    decreasing_lr = list(map(int, decreasing_lr.split(',')))\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        if epoch in decreasing_lr:\n",
    "            optimizer.param_groups[0]['lr'] *= 0.1\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            indx_target = target.clone()\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "                pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "                correct = pred.cpu().eq(indx_target).sum()\n",
    "                acc = correct * 1.0 / len(data)\n",
    "                print('Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f} lr: {:.2e}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    loss.data.item(), acc, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        elapse_time = time.time() - t_begin\n",
    "        speed_epoch = elapse_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * epochs - elapse_time\n",
    "        print(\"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "            elapse_time, speed_epoch, speed_batch, eta))\n",
    "        # misc.model_snapshot(model, os.path.join(args.logdir, 'latest.pth'))\n",
    "\n",
    "        if epoch % test_interval == 0:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            for data, target in test_loader:\n",
    "                indx_target = target.clone()\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                test_loss += F.cross_entropy(output, target).data.item()\n",
    "                pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "                correct += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "            test_loss = test_loss / len(test_loader) # average over number of mini-batch\n",
    "            acc = 100. * correct / len(test_loader.dataset)\n",
    "            print('\\tTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                test_loss, correct, len(test_loader.dataset), acc))\n",
    "            if acc > best_acc:\n",
    "                # new_file = os.path.join(args.logdir, 'best-{}.pth'.format(epoch))\n",
    "                # misc.model_snapshot(model, new_file, old_file=old_file, verbose=True)\n",
    "                best_acc = acc\n",
    "                # old_file = new_file\n",
    "    print(\"Total Elapse: {:.2f}, Best Result: {:.3f}%\".format(time.time()-t_begin, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data.item()\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    test_loss = test_loss / len(test_loader) # average over number of mini-batch\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\tTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest set: Average loss: 0.3789, Accuracy: 9379/10000 (94%)\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['features.0.weight_mask', 'features.1.running_mean', 'features.1.running_var', 'features.1.num_batches_tracked', 'features.3.weight_mask', 'features.4.running_mean', 'features.4.running_var', 'features.4.num_batches_tracked', 'features.7.weight_mask', 'features.8.running_mean', 'features.8.running_var', 'features.8.num_batches_tracked', 'features.10.weight_mask', 'features.11.running_mean', 'features.11.running_var', 'features.11.num_batches_tracked', 'features.14.weight_mask', 'features.15.running_mean', 'features.15.running_var', 'features.15.num_batches_tracked', 'features.17.weight_mask', 'features.18.running_mean', 'features.18.running_var', 'features.18.num_batches_tracked', 'features.21.weight_mask', 'features.22.running_mean', 'features.22.running_var', 'features.22.num_batches_tracked', 'classifier.0.weight_mask'])\n"
     ]
    }
   ],
   "source": [
    "prune_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest set: Average loss: 0.4045, Accuracy: 9172/10000 (92%)\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [20000/50000] Loss: 0.003458 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 0 [40000/50000] Loss: 0.002431 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 32.81s, 32.81 s/epoch, 0.13 s/batch, ets 295.32s\n",
      "\tTest set: Average loss: 0.3505, Accuracy: 9301/10000 (93%)\n",
      "Train Epoch: 1 [20000/50000] Loss: 0.001057 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 1 [40000/50000] Loss: 0.001426 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 71.25s, 35.63 s/epoch, 0.14 s/batch, ets 285.02s\n",
      "Train Epoch: 2 [20000/50000] Loss: 0.000681 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 2 [40000/50000] Loss: 0.001516 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 104.33s, 34.78 s/epoch, 0.14 s/batch, ets 243.44s\n",
      "Train Epoch: 3 [20000/50000] Loss: 0.001103 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 3 [40000/50000] Loss: 0.001624 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 137.42s, 34.36 s/epoch, 0.14 s/batch, ets 206.13s\n",
      "Train Epoch: 4 [20000/50000] Loss: 0.002847 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 4 [40000/50000] Loss: 0.000381 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 170.86s, 34.17 s/epoch, 0.14 s/batch, ets 170.86s\n",
      "Train Epoch: 5 [20000/50000] Loss: 0.000424 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 5 [40000/50000] Loss: 0.000198 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 204.48s, 34.08 s/epoch, 0.14 s/batch, ets 136.32s\n",
      "\tTest set: Average loss: 0.3372, Accuracy: 9331/10000 (93%)\n",
      "Train Epoch: 6 [20000/50000] Loss: 0.000673 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 6 [40000/50000] Loss: 0.000554 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 246.05s, 35.15 s/epoch, 0.14 s/batch, ets 105.45s\n",
      "Train Epoch: 7 [20000/50000] Loss: 0.000247 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 7 [40000/50000] Loss: 0.000503 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 280.10s, 35.01 s/epoch, 0.14 s/batch, ets 70.02s\n",
      "Train Epoch: 8 [20000/50000] Loss: 0.000822 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 8 [40000/50000] Loss: 0.000603 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 313.97s, 34.89 s/epoch, 0.14 s/batch, ets 34.89s\n",
      "Train Epoch: 9 [20000/50000] Loss: 0.002958 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 9 [40000/50000] Loss: 0.000177 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 348.00s, 34.80 s/epoch, 0.14 s/batch, ets 0.00s\n",
      "Total Elapse: 348.00, Best Result: 93.310%\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs=10, lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest set: Average loss: 0.3371, Accuracy: 9341/10000 (93%)\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['features.0.weight_mask', 'features.1.running_mean', 'features.1.running_var', 'features.1.num_batches_tracked', 'features.3.weight_mask', 'features.4.running_mean', 'features.4.running_var', 'features.4.num_batches_tracked', 'features.7.weight_mask', 'features.8.running_mean', 'features.8.running_var', 'features.8.num_batches_tracked', 'features.10.weight_mask', 'features.11.running_mean', 'features.11.running_var', 'features.11.num_batches_tracked', 'features.14.weight_mask', 'features.15.running_mean', 'features.15.running_var', 'features.15.num_batches_tracked', 'features.17.weight_mask', 'features.18.running_mean', 'features.18.running_var', 'features.18.num_batches_tracked', 'features.21.weight_mask', 'features.22.running_mean', 'features.22.running_var', 'features.22.num_batches_tracked', 'classifier.0.weight_mask'])\n"
     ]
    }
   ],
   "source": [
    "prune_model(model, 0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest set: Average loss: 0.3884, Accuracy: 9070/10000 (91%)\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [20000/50000] Loss: 0.003664 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 0 [40000/50000] Loss: 0.005356 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 33.06s, 33.06 s/epoch, 0.13 s/batch, ets 297.51s\n",
      "\tTest set: Average loss: 0.3214, Accuracy: 9282/10000 (93%)\n",
      "Train Epoch: 1 [20000/50000] Loss: 0.007936 Acc: 0.9950 lr: 1.00e-05\n",
      "Train Epoch: 1 [40000/50000] Loss: 0.001997 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 73.73s, 36.87 s/epoch, 0.15 s/batch, ets 294.94s\n",
      "Train Epoch: 2 [20000/50000] Loss: 0.004576 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 2 [40000/50000] Loss: 0.005964 Acc: 0.9950 lr: 1.00e-05\n",
      "Elapsed 108.01s, 36.00 s/epoch, 0.14 s/batch, ets 252.03s\n",
      "Train Epoch: 3 [20000/50000] Loss: 0.002531 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 3 [40000/50000] Loss: 0.002713 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 142.42s, 35.61 s/epoch, 0.14 s/batch, ets 213.63s\n",
      "Train Epoch: 4 [20000/50000] Loss: 0.006665 Acc: 0.9950 lr: 1.00e-05\n",
      "Train Epoch: 4 [40000/50000] Loss: 0.002761 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 176.80s, 35.36 s/epoch, 0.14 s/batch, ets 176.80s\n",
      "Train Epoch: 5 [20000/50000] Loss: 0.002871 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 5 [40000/50000] Loss: 0.001338 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 211.58s, 35.26 s/epoch, 0.14 s/batch, ets 141.05s\n",
      "\tTest set: Average loss: 0.3055, Accuracy: 9301/10000 (93%)\n",
      "Train Epoch: 6 [20000/50000] Loss: 0.003068 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 6 [40000/50000] Loss: 0.002541 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 253.09s, 36.16 s/epoch, 0.14 s/batch, ets 108.47s\n",
      "Train Epoch: 7 [20000/50000] Loss: 0.001746 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 7 [40000/50000] Loss: 0.005421 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 287.83s, 35.98 s/epoch, 0.14 s/batch, ets 71.96s\n",
      "Train Epoch: 8 [20000/50000] Loss: 0.001406 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 8 [40000/50000] Loss: 0.002295 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 322.83s, 35.87 s/epoch, 0.14 s/batch, ets 35.87s\n",
      "Train Epoch: 9 [20000/50000] Loss: 0.002364 Acc: 1.0000 lr: 1.00e-05\n",
      "Train Epoch: 9 [40000/50000] Loss: 0.007663 Acc: 1.0000 lr: 1.00e-05\n",
      "Elapsed 357.99s, 35.80 s/epoch, 0.14 s/batch, ets 0.00s\n",
      "Total Elapse: 357.99, Best Result: 93.010%\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs=10, lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
